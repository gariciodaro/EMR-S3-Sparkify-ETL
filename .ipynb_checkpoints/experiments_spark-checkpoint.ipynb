{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "from datetime import datetime\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.functions import year, month, dayofmonth, hour, weekofyear, date_format,dayofweek\n",
    "\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "#config.read('dl.cfg')\n",
    "config.read('/home/gari/.aws/credentials')\n",
    "os.environ['AWS_ACCESS_KEY_ID']=config.get('credentials','KEY')\n",
    "os.environ['AWS_SECRET_ACCESS_KEY']=config.get('credentials','SECRET')\n",
    "os.environ['AWS_SECRET_ACCESS_KEY']=config.get('credentials','SECRET')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_spark_session():\n",
    "    spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:2.7.0\") \\\n",
    "        .getOrCreate()\n",
    "    return spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = create_spark_session()\n",
    "input_data = \"s3a://udacity-dend/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "aa=\"\"\"\n",
    " |-- artist: string (nullable = true)\n",
    " |-- auth: string (nullable = true)\n",
    " |-- firstName: string (nullable = true)\n",
    " |-- gender: string (nullable = true)\n",
    " |-- itemInSession: long (nullable = true)\n",
    " |-- lastName: string (nullable = true)\n",
    " |-- length: double (nullable = true)\n",
    " |-- level: string (nullable = true)\n",
    " |-- location: string (nullable = true)\n",
    " |-- method: string (nullable = true)\n",
    " |-- page: string (nullable = true)\n",
    " |-- registration: long (nullable = true)\n",
    " |-- sessionId: long (nullable = true)\n",
    " |-- song: string (nullable = true)\n",
    " |-- status: long (nullable = true)\n",
    " |-- ts: long (nullable = true)\n",
    " |-- userAgent: string (nullable = true)\n",
    " |-- userId: string (nullable = true)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'aa' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-5ed1366f2c52>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnew_aa\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0meach\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;32mfor\u001b[0m \u001b[0meach\u001b[0m \u001b[0;32min\u001b[0m \u001b[0maa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\":\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mnew_aa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'aa' is not defined"
     ]
    }
   ],
   "source": [
    "new_aa=[each.split(\" \")[-1]for each in aa.split(\":\")]\n",
    "new_aa.pop(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#exprs= [StructField(\"'{}'\".format(field),StringType()) for field in new_aa]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[StructField('artist',StringType,true),\n",
       " StructField('auth',StringType,true),\n",
       " StructField('firstName',StringType,true),\n",
       " StructField('gender',StringType,true),\n",
       " StructField('itemInSession',StringType,true),\n",
       " StructField('lastName',StringType,true),\n",
       " StructField('length',StringType,true),\n",
       " StructField('level',StringType,true),\n",
       " StructField('location',StringType,true),\n",
       " StructField('method',StringType,true),\n",
       " StructField('page',StringType,true),\n",
       " StructField('registration',StringType,true),\n",
       " StructField('sessionId',StringType,true),\n",
       " StructField('song',StringType,true),\n",
       " StructField('status',StringType,true),\n",
       " StructField('ts',StringType,true),\n",
       " StructField('userAgent',StringType,true),\n",
       " StructField('userId',StringType,true)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exprs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField , DoubleType, StringType, IntegerType, DateType\n",
    "#songSchema = StructType(exprs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField('artist',StringType,true),StructField('auth',StringType,true),StructField('firstName',StringType,true),StructField('gender',StringType,true),StructField('itemInSession',StringType,true),StructField('lastName',StringType,true),StructField('length',StringType,true),StructField('level',StringType,true),StructField('location',StringType,true),StructField('method',StringType,true),StructField('page',StringType,true),StructField('registration',StringType,true),StructField('sessionId',StringType,true),StructField('song',StringType,true),StructField('status',StringType,true),StructField('ts',StringType,true),StructField('userAgent',StringType,true),StructField('userId',StringType,true)))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "songSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.json(\"/home/gari/Desktop/myGItRepos/project_spark/data/song_data/*/*/*/*.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_data_pandas=df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_data_pandas.to_csv(\"./song_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+---------------+--------------------+----------------+--------------------+---------+---------+------------------+--------------------+----+\n",
      "|         artist_id|artist_latitude|     artist_location|artist_longitude|         artist_name| duration|num_songs|           song_id|               title|year|\n",
      "+------------------+---------------+--------------------+----------------+--------------------+---------+---------+------------------+--------------------+----+\n",
      "|ARDR4AC1187FB371A1|           null|                    |            null|Montserrat Caball...|511.16363|        1|SOBAYLL12A8C138AF9|Sono andati? Fing...|   0|\n",
      "|AREBBGV1187FB523D2|           null|         Houston, TX|            null|Mike Jones (Featu...|173.66159|        1|SOOLYAZ12A6701F4A6|Laws Patrolling (...|   0|\n",
      "|ARMAC4T1187FB3FA4C|       40.82624|   Morris Plains, NJ|       -74.47995|The Dillinger Esc...|207.77751|        1|SOBBUGU12A8C13E95D|Setting Fire to S...|2004|\n",
      "|ARPBNLO1187FB3D52F|       40.71455|        New York, NY|       -74.00712|            Tiny Tim| 43.36281|        1|SOAOIBZ12AB01815BE|I Hold Your Hand ...|2000|\n",
      "|ARNF6401187FB57032|       40.79086|New York, NY [Man...|       -73.96644|   Sophie B. Hawkins|  305.162|        1|SONWXQJ12A8C134D94|The Ballad Of Sle...|1994|\n",
      "+------------------+---------------+--------------------+----------------+--------------------+---------+---------+------------------+--------------------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import  pyspark.sql.functions as F\n",
    "#df = df.withColumn(\"ts\", F.to_timestamp(\"ts\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"song_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- artist_id: string (nullable = true)\n",
      " |-- artist_latitude: double (nullable = true)\n",
      " |-- artist_location: string (nullable = true)\n",
      " |-- artist_longitude: double (nullable = true)\n",
      " |-- artist_name: string (nullable = true)\n",
      " |-- duration: double (nullable = true)\n",
      " |-- num_songs: long (nullable = true)\n",
      " |-- song_id: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- year: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------------+------------------+----+---------+\n",
      "|           song_id|               title|         artist_id|year| duration|\n",
      "+------------------+--------------------+------------------+----+---------+\n",
      "|SOBAYLL12A8C138AF9|Sono andati? Fing...|ARDR4AC1187FB371A1|   0|511.16363|\n",
      "|SOOLYAZ12A6701F4A6|Laws Patrolling (...|AREBBGV1187FB523D2|   0|173.66159|\n",
      "|SOBBUGU12A8C13E95D|Setting Fire to S...|ARMAC4T1187FB3FA4C|2004|207.77751|\n",
      "|SOAOIBZ12AB01815BE|I Hold Your Hand ...|ARPBNLO1187FB3D52F|2000| 43.36281|\n",
      "|SONWXQJ12A8C134D94|The Ballad Of Sle...|ARNF6401187FB57032|1994|  305.162|\n",
      "|SONYPOM12A8C13B2D7|I Think My Wife I...|ARDNS031187B9924F0|2005|186.48771|\n",
      "|SODREIN12A58A7F2E5|A Whiter Shade Of...|ARLTWXK1187FB5A3F8|   0|326.00771|\n",
      "|SOWQTQZ12A58A7B63E|Streets On Fire (...|ARPFHN61187FB575F6|   0|279.97995|\n",
      "|SODUJBS12A8C132150|Wessex Loses a Bride|ARI2JSK1187FB496EF|   0|111.62077|\n",
      "|SOZHPGD12A8C1394FE|     Baby Come To Me|AR9AWNF1187B9AB0B4|   0|236.93016|\n",
      "+------------------+--------------------+------------------+----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT song_id,\n",
    "        title,\n",
    "        artist_id,\n",
    "        year,\n",
    "        duration\n",
    "        FROM song_data\n",
    "\"\"\").limit(10).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "songs_table=df.select(\"song_id\",\"title\",\"artist_id\",\"year\",\"duration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#output_data = \"s3n://projectsparkify/\"\n",
    "#songs_table.write.partitionBy(\"year\",\"artist_id\").parquet(\"s3a://projectsparkify2/data/songs_table.parquet\")\n",
    "songs_table.write.partitionBy(\"year\",\"artist_id\").parquet(\"./parquet_area/songs_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "artists_table = df.select(\"artist_id\",\"artist_name\",\"artist_location\",\"artist_latitude\",\"artist_longitude\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "artists_table.write.parquet(\"./parquet_area/artists_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_data =\"./data/log-data/*.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df =  spark.read.json(log_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_log_data=  df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_log_data.to_csv(\"./log_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- artist: string (nullable = true)\n",
      " |-- auth: string (nullable = true)\n",
      " |-- firstName: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- itemInSession: long (nullable = true)\n",
      " |-- lastName: string (nullable = true)\n",
      " |-- length: double (nullable = true)\n",
      " |-- level: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- method: string (nullable = true)\n",
      " |-- page: string (nullable = true)\n",
      " |-- registration: double (nullable = true)\n",
      " |-- sessionId: long (nullable = true)\n",
      " |-- song: string (nullable = true)\n",
      " |-- status: long (nullable = true)\n",
      " |-- ts: long (nullable = true)\n",
      " |-- userAgent: string (nullable = true)\n",
      " |-- userId: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.filter(df.page == 'NextSong')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_table =df.select(\"userId\",\"firstName\",\"lastName\",\"gender\",\"level\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_table.write.parquet(\"./parquet_area/users_table\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_timestamp\n",
    "#to_timestamp(col(\"input_timestamp\"))\n",
    "df = df.withColumn(\"timestamp\", to_timestamp(col(\"ts\")/1000.0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+-------------+\n",
      "|timestamp              |ts           |\n",
      "+-----------------------+-------------+\n",
      "|2018-11-15 01:30:26.796|1542241826796|\n",
      "|2018-11-15 01:41:21.796|1542242481796|\n",
      "+-----------------------+-------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"timestamp\",\"ts\").show(2, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_datetime = udf(lambda x: datetime.datetime.fromtimestamp(x))\n",
    "get_hour = udf(lambda x: x.hour)\n",
    "df =  df.withColumn(\"datetime\", get_hour(col(\"timestamp\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|datetime|\n",
      "+--------+\n",
      "|1       |\n",
      "|1       |\n",
      "+--------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"datetime\").show(2, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read log data file\n",
    "df =  spark.read.json(log_data)\n",
    "\n",
    "# filter by actions for song plays\n",
    "df = df.filter(df.page == 'NextSong')\n",
    "\n",
    "# extract columns for users table    \n",
    "users_table =df.selectExpr(\"userId as user_id\",\n",
    "                        \"firstName as first_name\",\n",
    "                        \"lastName as last_name\",\n",
    "                        \"gender\",\"level\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+---------+------+-----+\n",
      "|user_id|first_name|last_name|gender|level|\n",
      "+-------+----------+---------+------+-----+\n",
      "|     26|      Ryan|    Smith|     M| free|\n",
      "|     26|      Ryan|    Smith|     M| free|\n",
      "|     26|      Ryan|    Smith|     M| free|\n",
      "+-------+----------+---------+------+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "users_table.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_timestamp = udf(lambda x: int(x),IntegerType())\n",
    "df = df.withColumn(\"start_time\", get_timestamp(col(\"ts\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------+\n",
      "|timestamp|           ts|\n",
      "+---------+-------------+\n",
      "|348567532|1542241826796|\n",
      "|349222532|1542242481796|\n",
      "+---------+-------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"timestamp\",\"ts\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql.types import StructType, StructField , DoubleType, StringType, IntegerType, DateType,TimestampType\n",
    "#get_datetime = udf(lambda x: from_unixtime(x), TimestampType())\n",
    "#df = df.withColumn('datetime2', from_unixtime('timestamp'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_datetime = udf(lambda x: datetime.fromtimestamp(x/1000),DateType())\n",
    "df =  df.withColumn(\"datetime\", get_datetime(col(\"ts\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+----------+\n",
      "|start_time|           ts|  datetime|\n",
      "+----------+-------------+----------+\n",
      "| 348567532|1542241826796|2018-11-15|\n",
      "| 349222532|1542242481796|2018-11-15|\n",
      "+----------+-------------+----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"start_time\",\"ts\",\"datetime\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_data = input_data+\"/log-data/*.json\"\n",
    "\n",
    "# read log data file\n",
    "df =  spark.read.json(log_data)\n",
    "\n",
    "# filter by actions for song plays\n",
    "df = df.filter(df.page == 'NextSong')\n",
    "\n",
    "get_timestamp = udf(lambda x: int(x),IntegerType())\n",
    "df = df.withColumn(\"start_time\", get_timestamp(col(\"ts\")))\n",
    "# get timestamp on date time\n",
    "# df = df.withColumn(\"timestamp\", to_timestamp(col(\"ts\")/1000.0))\n",
    "\n",
    "# create datetime column from original timestamp column\n",
    "get_datetime = udf(lambda x: datetime.fromtimestamp(x/1000),DateType())\n",
    "df =  df.withColumn(\"datetime\", get_datetime(col(\"ts\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "#time_table.withColumn('hour', functions.dayofweek('datetime'))\n",
    "\n",
    "#from pyspark.sql.functions import year, month, dayofmonth, hour, weekofyear, date_format, weekday\n",
    "\n",
    "\n",
    "dict_time_func={\"hour\":functions.hour,\n",
    "                \"day\":functions.dayofmonth,\n",
    "                \"week\":functions.weekofyear,\n",
    "                \"month\":functions.month,\n",
    "                \"year\":functions.year ,\n",
    "                \"weekday\":functions.dayofweek}\n",
    "\n",
    "time_table = df.select(\"start_time\",\"datetime\")\n",
    "\n",
    "for key,value in dict_time_func.items():\n",
    "    time_table = time_table.withColumn(key,value(\"datetime\"))\n",
    "    #time_table.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----+---+----+-----+----+-------+\n",
      "|start_time|  datetime|hour|day|week|month|year|weekday|\n",
      "+----------+----------+----+---+----+-----+----+-------+\n",
      "| 348567532|2018-11-15|   0| 15|  46|   11|2018|      5|\n",
      "| 349222532|2018-11-15|   0| 15|  46|   11|2018|      5|\n",
      "+----------+----------+----+---+----+-----+----+-------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "time_table.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'day'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-172-e75097716bfe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0myear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmonth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdayofmonth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhour\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweekofyear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdate_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdayofweek\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mday\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'day'"
     ]
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----+\n",
      "|start_time|  datetime|hour|\n",
      "+----------+----------+----+\n",
      "| 348567532|2018-11-15|   5|\n",
      "| 349222532|2018-11-15|   5|\n",
      "+----------+----------+----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#functions.hour(\"datetime\")\n",
    "#df.where(col(\"dt_mvmt\").isNotNull())\n",
    "#get_hour = udf(lambda x: functions.hour(x) )\n",
    "\n",
    "#time_table.filter(col(\"datetime\").isNotNull()).withColumn(\"hour_tes\",get_hour(col(\"datetime\"))).show(2)\n",
    "\n",
    "time_table.withColumn('hour', functions.dayofweek('datetime')).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|  datetime|\n",
      "+----------+\n",
      "|2018-11-15|\n",
      "|2018-11-15|\n",
      "+----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "time_table.select(\"datetime\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6820"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_table.filter(col(\"datetime\").isNotNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
