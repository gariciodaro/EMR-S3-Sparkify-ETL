{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETL dev\n",
    "Local execution of spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "from datetime import datetime\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.functions import year, month, dayofmonth, hour\n",
    "from pyspark.sql.functions import  weekofyear, date_format,dayofweek,countDistinct\n",
    "from pyspark.sql.functions import  monotonically_increasing_id\n",
    "\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "#config.read('dl.cfg')\n",
    "config.read('/home/gari/.aws/credentials')\n",
    "os.environ['AWS_ACCESS_KEY_ID']=config.get('credentials','KEY')\n",
    "os.environ['AWS_SECRET_ACCESS_KEY']=config.get('credentials','SECRET')\n",
    "os.environ['AWS_SECRET_ACCESS_KEY']=config.get('credentials','SECRET')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_spark_session():\n",
    "    spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:2.7.0\") \\\n",
    "        .getOrCreate()\n",
    "    return spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = create_spark_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField , DoubleType, StringType, IntegerType, DateType\n",
    "#songSchema = StructType(exprs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# def process_song_data(spark, input_data, output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.json(\"/home/gari/Desktop/myGItRepos/project_spark/data/song_data/*/*/*/*.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data='/home/gari/Desktop/myGItRepos/project_spark/data'\n",
    "output_data='./parquet_area/'\n",
    "song_data = input_data+\"/song_data/*/*/*/*.json\"\n",
    "\n",
    "# read song data file\n",
    "df = spark.read.json(song_data)\n",
    "\n",
    "# extract columns to create songs table\n",
    "songs_table = df.select(\"song_id\",\n",
    "                        \"title\",\n",
    "                        \"artist_id\",\n",
    "                        \"year\",\n",
    "                        \"duration\").drop_duplicates(subset=['song_id'])\n",
    "\n",
    "# write songs table to parquet files partitioned by year and artist\n",
    "songs_table.write.mode(\"overwrite\").partitionBy(\"year\",\"artist_id\").parquet(output_data+\"songs\")\n",
    "\n",
    "# extract columns to create artists table\n",
    "artists_table = df.selectExpr(\"artist_id\",\n",
    "                            \"artist_name as name\",\n",
    "                            \"artist_location as location\",\n",
    "                            \"artist_latitude as latitude\",\"artist_longitude as longitude\")\n",
    "\n",
    "artists_table = artists_table.drop_duplicates(subset=['artist_id'])\n",
    "\n",
    "\n",
    "# write artists table to parquet files\n",
    "artists_table.write.mode(\"overwrite\").parquet(output_data+\"artists\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# def process_log_data(spark, input_data, output_data):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_data = input_data+\"/log-data/*.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# read log data file\n",
    "df =  spark.read.json(log_data)\n",
    "\n",
    "# filter by actions for song plays\n",
    "df = df.filter(df.page == 'NextSong')\n",
    "\n",
    "# extract columns for users table    \n",
    "users_table =df.selectExpr(\"userId as user_id\",\n",
    "                            \"firstName as first_name\",\n",
    "                            \"lastName as last_name\",\n",
    "                            \"gender\",\"level\").drop_duplicates(subset=['user_id'])\n",
    "\n",
    "# write users table to parquet files\n",
    "users_table.write.mode(\"overwrite\").parquet(output_data+\"users\")\n",
    "# create timestamp column from original timestamp column\n",
    "get_timestamp = udf(lambda x: int(x),IntegerType())\n",
    "df = df.withColumn(\"start_time\", get_timestamp(col(\"ts\")/1000.0)).drop_duplicates(subset=['ts'])\n",
    "# get timestamp on date time\n",
    "# df = df.withColumn(\"timestamp\", to_timestamp(col(\"ts\")/1000.0))\n",
    "\n",
    "# create datetime column from original timestamp column\n",
    "get_datetime = udf(lambda x: datetime.fromtimestamp(x/1000),DateType())\n",
    "df =  df.withColumn(\"datetime\", get_datetime(col(\"ts\")))\n",
    "\n",
    "# extract columns to create time table\n",
    "time_table = df.select(\"start_time\",\"datetime\")\n",
    "\n",
    "dict_time_func={\"hour\":hour,\n",
    "            \"day\":dayofmonth,\n",
    "            \"week\":weekofyear,\n",
    "            \"month\":month,\n",
    "            \"year\":year ,\n",
    "            \"weekday\":dayofweek}\n",
    "\n",
    "\n",
    "for key,value in dict_time_func.items():\n",
    "    time_table = time_table.withColumn(key,value(\"datetime\"))\n",
    "    \n",
    "time_table.write.mode(\"overwrite\").partitionBy(\"year\",\"month\").parquet(output_data+\"time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_table_read    = spark.read.parquet(output_data+\"songs\")[\"song_id\",\"title\",\"artist_id\",\"duration\"]\n",
    "artists_table_read = spark.read.parquet(output_data+\"artists\")[\"artist_id\",\"name\"]\n",
    "df_song_comp = song_table_read.join(artists_table_read, on=['artist_id'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------------+------------------+---------+\n",
      "|           song_id|               title|         artist_id| duration|\n",
      "+------------------+--------------------+------------------+---------+\n",
      "|SOAOIBZ12AB01815BE|I Hold Your Hand ...|ARPBNLO1187FB3D52F| 43.36281|\n",
      "|SONYPOM12A8C13B2D7|I Think My Wife I...|ARDNS031187B9924F0|186.48771|\n",
      "+------------------+--------------------+------------------+---------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "song_table_read.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# extract columns from joined song and log datasets to create songplays table \n",
    "songplays_aux_table = df.selectExpr(\"song as title\",\"artist as name\",\"length as duration\",\n",
    "                                \"start_time\",\n",
    "                                \"userId as user_id\",\n",
    "                                \"level\",\n",
    "                                \"sessionId as session_id\",\n",
    "                                \"location\",\n",
    "                                \"userAgent as user_agent\").drop_duplicates(subset=['start_time'])\n",
    "\n",
    "songplays_aux_table = songplays_aux_table.withColumn(\"songplay_id\", monotonically_increasing_id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "songplays_table = songplays_aux_table.join(df_song_comp, on=['title','name','duration'], how='left')\n",
    "songplays_table = songplays_table.selectExpr('songplay_id',\n",
    "                                             'start_time',\n",
    "                                             'user_id',\n",
    "                                             'level',\n",
    "                                             'song_id',\n",
    "                                             'artist_id',\n",
    "                                             'session_id',\n",
    "                                             'location',\n",
    "                                             'user_agent')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "songplays_table.write.mode(\"overwrite\").parquet(output_data+\"songplays\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+-------+-----+------------------+------------------+----------+--------------------+--------------------+\n",
      "| songplay_id|start_time|user_id|level|           song_id|         artist_id|session_id|            location|          user_agent|\n",
      "+------------+----------+-------+-----+------------------+------------------+----------+--------------------+--------------------+\n",
      "|627065225236|1542837407|     15| paid|SOZCTXZ12AB0182364|AR5KOSW1187FB35FF4|       818|Chicago-Napervill...|\"Mozilla/5.0 (X11...|\n",
      "+------------+----------+-------+-----+------------------+------------------+----------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "songplays_table.where(col(\"song_id\").isNotNull()).show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------+---------+\n",
      "|               title|          name| duration|\n",
      "+--------------------+--------------+---------+\n",
      "|Make Love To Your...|  Bill Withers|383.73832|\n",
      "|All Hands Against...|The Black Keys|196.91057|\n",
      "| I'm Still Breathing|    Katy Perry|228.49261|\n",
      "+--------------------+--------------+---------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "songplays_aux_table.select('title','name','duration').show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+---------+\n",
      "|               title|       name| duration|\n",
      "+--------------------+-----------+---------+\n",
      "|I Hold Your Hand ...|   Tiny Tim| 43.36281|\n",
      "|I Think My Wife I...| Tim Wilson|186.48771|\n",
      "|A Whiter Shade Of...|King Curtis|326.00771|\n",
      "+--------------------+-----------+---------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_song_comp.select('title','name','duration').show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
